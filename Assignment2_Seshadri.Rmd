---
title: 'Assignment 2: Regression Model Building'
author: "Sri Seshadri"
date: "7/1/2017"
output: 
  pdf_document: 
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

## 1. Introduction

This report discusses the regression models for estimating or predicting the sales price of "typical" homes in Ames, Iowa. 

```{r Read data, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE}
ames <- readr::read_delim(file = 'ames_housing_data.csv', delim = ",")
# chamge from scientic notations, to restore to default options(scipen = 0)
options(scipen = 999)
library(magrittr)
```


```{r Sample Frame, echo = FALSE, message=FALSE, warning=FALSE, tidy=TRUE}
LivingAreaCutoff <- 800
# Adding drop conditions varaible
# insert dummy variable to code SaleCondition being either equal to 'Normal' or 'Partial'
ames$Sale_NrmPar <- ifelse(ames$SaleCondition == 'Normal' | ames$SaleCondition == 'Partial', 1,0)
ames$DropCondition <- ifelse(ames$Zoning!='RL','01: Not LowDensityZone',
	ifelse(ames$Sale_NrmPar == 0,'02: Not Normal/Partial Sale',
	ifelse(ames$Street!='Pave','03: Street Not Paved',
	ifelse(ames$GrLivArea <LivingAreaCutoff,'04: Less than 800 SqFt',
	 ifelse(ames$TotalBsmtSF <1,'05: No Basement',      
	'99: Eligible Sample')
	))))

# Waterfall
waterfall <- ames %>% 
  dplyr::group_by(DropCondition) %>% 
  dplyr::summarise(counts=n())

```


## 2. Sample definition

It is assumed that typical home buyers are those that move from apartments to single family or town homes. Also apartments are less likely to be sold to individuals as they remain holdings of owners for rental income. Single family and town homes belong to "Residential Low density" (RL) zoning classification in the city of Ames. Data belonging to only to the RL zone is considered for analysis and model development. Also, it is assumed that typical homes have paved streets for access and above grade living area greater than `r LivingAreaCutoff` square feet. Sales data belonging to homes that were sold in abnormal conditions such as trade in, foreclosure or short sale are not included in the analysis. Also, sales between family members, sale of adjoining lot, linked properties are omitted from the data. Homes with no basements are excluded from the analysis at this time. Table 1 shows the waterfall of the data not included in the data and the eligible samples.

```{r Waterfall, echo=FALSE, warning = F, message=F, tidy=T}
# Print waterfall table
knitr::kable(waterfall, align = c("l", "r"),caption = "Drop waterfall")
# Define training portion of the data
trainPercent <- round(0.7,1)
# Columns of interest
colsofinterest <- c('SID'
  ,'LotArea'
, 'LotConfig'
, 'Neighborhood'
, 'BldgType'
, 'HouseStyle'
, 'OverallCond'
, 'YearRemodel'
, 'TotalBsmtSF'
, 'GrLivArea'
, 'BsmtFullBath'
, 'BsmtHalfBath'
, 'FullBath'
, 'HalfBath'
, 'BedroomAbvGr'
, 'KitchenQual'
, 'TotRmsAbvGrd'
, 'GarageArea'
, 'MoSold'
, 'YrSold'
, 'SaleCondition'
, 'SalePrice')

```


### 2.1 Variables of interest for modelling

The following variables in the data were deemed to be of interest for model building. The choice of parameters was based upon intial Exploratory Data Analysis (EDA) and subject matter expertise. See appendix A.1 for data quality checks.

```{r SampleFrame, echo = F, warning = F, message=F, tidy=T}
# Cleanly show the columns of interest in pdf. Making the colsofinterest as matrix for easy printing.
colsmatrix <- matrix(colsofinterest[2:length(colsofinterest)],ncol = 3)
# printing on pdf
knitr::kable(colsmatrix, caption = "Variables of interest")
```


\pagebreak

### 2.2 Training and validation samples.

From the eligible samples, `r paste0(trainPercent*100,"%")` of the data is randomly samples to be used as the dataset on which model is developed from. This dataset would be refered to as training dataset. The remaining `r paste0((1-trainPercent)*100,"%")` is used as the validation set to evaluate the model performance of predicting sale price on data that is outside the training set. Table 3 shows the split of the total eligible samples.  


```{r TrainingSet, echo=F, warning=F, message=F, tidy=T}
# Get sample frame.
SampleFrame <- ames %>% 
  dplyr::filter(DropCondition == '99: Eligible Sample') %>% 
  dplyr::select_(.dots = colsofinterest)
SampleFrame <- SampleFrame %>% 
  dplyr::mutate(TotalBath = BsmtFullBath + BsmtHalfBath + FullBath + HalfBath)
# training set
train <- dplyr::sample_n(SampleFrame,size = trainPercent*nrow(SampleFrame), replace = F,set.seed(2000))
train <- train %>% dplyr::arrange(SID)
# Validation set
Validation <- dplyr::sample_n(SampleFrame,size = (1-trainPercent)*nrow(SampleFrame), replace = F, set.seed(2000))
Validation <- Validation %>% dplyr::arrange(SID)
# Check row counts
df <- cbind(Data = c("Training set", "Validation set"), Samples = c(nrow(train),nrow(Validation)))
knitr::kable(df,align = c("l","r"),caption = "Training and Validation sampling")
```

## 3. Exploratory Data Analysis (EDA)

For exploratory analysis, entire sample frame is used, so any anomalies that were missed surfaces in this process. While EDA by itself doesn not become an data cleaning step, but certainly allows the opportunity to identify issues in the data. In this section we will focus only on the continuous variable. Exploratory analysis on categorical variables can be found in https://github.com/srivathsesh/RegressionAnalysis/blob/master/Assignment1_Seshadri.pdf.

### 3.1 EDA of continuous variables
Is is hypothesized that bigger the house, more likely is higher occupancy and sale value. The higher occupancy means a likely higher lot area, living area, basement area, garage space for parking, total number of rooms and baths. Also it is hypothesized that newer homes are likely to be more valued than the older homes. Before exploring each of the potential predictors, it will be useful to see if there is multicollinearity amongst the predictors. Figure 1 shows, potential relationship between Above grade living area, Sale price and Total basment area. The lot area may have a steeper slope with sale price. This warrants a closer look.  

```{r Multicolinearity, echo=F,warning=F,message=F,tidy=T,fig.cap="Multicolinearity check amongst potential predictors"}
# Multicolinearity exploration
test <- SampleFrame[,c('SalePrice','TotalBsmtSF','GarageArea', 'GrLivArea', 'LotArea', 'TotalBath')]
pairs(test)
```  

Figure 2 shows the relationship between total basement area and the sale price. There are few outliers in the basement area. To explore the relationship better, data corresponding to basement area above 2500 square feet is removed the plot. It is seen that total basement area seens very promising predictor. It may be used along with other predictors to model sales price.  
  
  

```{r Sales vs BasementArea, echo=FALSE, fig.cap="Sale price vs Basement area", fig.height=3, message=FALSE, warning=FALSE}
# Plot of basement area vs sale price
library(ggplot2)
library(gridExtra)
BasementArea <- ggplot(data = SampleFrame,mapping = aes(x = TotalBsmtSF, y = SalePrice)) + geom_point() + geom_smooth(method = "lm", se = T) + xlab('Toal basement area') + theme_bw()
# annotation addition
BasementArea <- BasementArea + annotate("text", x = 2000, y = 700000, label = paste0("correlation = ",round(cor(SampleFrame$TotalBsmtSF,SampleFrame$SalePrice),2)))
# Restrict basment to < 2500
RestrictedBsmtSF <- SampleFrame %>% 
  dplyr::filter(TotalBsmtSF > 0 & TotalBsmtSF < 2500)
# Basement < 2500 vs SalePrice
RestrictedBasement <- ggplot(data = RestrictedBsmtSF,mapping = aes(x = TotalBsmtSF, y = SalePrice)) + geom_point() + geom_smooth(method = "lm", se = T) + xlab('Toal basement area') + theme_bw()
# Annotations
RestrictedBasement <- RestrictedBasement +  annotate("text", x = 1500, y = 700000, label = paste0("correlation = ",round(cor(SampleFrame$TotalBsmtSF,SampleFrame$SalePrice),2)))
# Print plot
grid.arrange(BasementArea, RestrictedBasement, ncol = 2)
```  

\pagebreak

Figure 3 shows the relationship between Living area and Total basement area as well as Sale price. There seem to be a linear relationship between living area and sale price when living area is less than 4000 square feet. Likewise with Garage area when its greater than zero and less than 1000 square feet. Figure 4 explores if there is a relationship between Garage Area, Living area and Total basement area. There is a weak correaltion between living area and total basement area.

```{r Living Area vs Sale Price vs Garage Area, echo=F,warning=F,message=F,tidy=T,fig.height=3,fig.cap=" Sales Price vs Garage Area and LivingArea"}
LivingArea <- ggplot(SampleFrame) + geom_point(mapping = aes(x = GrLivArea, y = SalePrice)) + xlab('Living Area') + geom_smooth(mapping = aes(x = GrLivArea, y = SalePrice), se = T) + theme_bw()

GargeArea<- ggplot(SampleFrame) + geom_point(mapping = aes(x = GarageArea, y = SalePrice)) + xlab('Garage Area') + geom_smooth(mapping = aes(x = GarageArea, y = SalePrice), se = T) + theme_bw()

RestrictedLivingGarageArea <- SampleFrame %>% 
  dplyr::filter(GrLivArea < 4000 & GarageArea > 0 & GarageArea < 1000)

LivingAreaRestricted <- ggplot(RestrictedLivingGarageArea) + geom_point(mapping = aes(x = GrLivArea, y = SalePrice)) + xlab('Living Area') + geom_smooth(mapping = aes(x = GrLivArea, y = SalePrice), se = T) + theme_bw()

GargeAreaRestricted<- ggplot(RestrictedLivingGarageArea) + geom_point(mapping = aes(x = GarageArea, y = SalePrice)) + xlab('Garage Area') + geom_smooth(mapping = aes(x = GarageArea, y = SalePrice), se = T) + theme_bw()


grid.arrange(LivingArea, GargeArea,LivingAreaRestricted,GargeAreaRestricted, ncol = 2)

```  


```{r, echo=F,message=F, warning=F,tidy=T, fig.height=4,fig.cap= "GarageArea vs Living and Basement area", fig.height=3}

RestGarageVsLivArea <- ggplot(RestrictedLivingGarageArea) + geom_point(mapping = aes(x = GarageArea, y = GrLivArea)) + xlab('Garage Area') + geom_smooth(mapping = aes(x = GarageArea, y = GrLivArea), se = T) + theme_bw()

RestGarageVsLivArea <- RestGarageVsLivArea + annotate("text",x = 250, y = 3000,label = paste0("Correlation = ",round(cor(RestrictedLivingGarageArea$GarageArea,RestrictedLivingGarageArea$GrLivArea),2)))

RestGarageVsBsmt <- ggplot(RestrictedLivingGarageArea) + geom_point(mapping = aes(x = GarageArea, y = TotalBsmtSF)) + xlab('Garage Area') + geom_smooth(mapping = aes(x = GarageArea, y = TotalBsmtSF), se = T) + theme_bw()

RestGarageVsBsmt <- RestGarageVsBsmt + annotate("text",x = 250, y = 3000,label = paste0("Correlation = ",round(cor(RestrictedLivingGarageArea$GarageArea,RestrictedLivingGarageArea$TotalBsmtSF),2)))

grid.arrange(RestGarageVsLivArea,RestGarageVsBsmt, nrow=2)

```  

\pagebreak
  
  
## 4. Simple Linear Regression Models

The Above grade living area and total basement seem to be good two candidates for building a regression model. The regression models would built on the training data set. However, after the EDA, it seems best to remove samples that correspond to above grade living area of 4000 square feet or greater and garage area greater than 1000 square feet. While it is best to update the drop conditions in section 2, at this time, the additional drop conditions would be applied to the training set. The training set data with new drop conditions applied is called "trainFiltered" (The R output reference this as the data)"

### 4.1 Simple linear regression model with "above grade living area"" as predictor

The model fit results for a single linear regression model with Living area above grade as predictor is seen below. The intercept = 0 hypothesis if failed to be rejected. It would be appropriate to fit a no intercept model. From a goodness of fit perspective, the residuals are of mean 0 and distributed fairly normally from a "thick pen test" of the Q-Q plot. However, the residulas are not homoscedastic, the residuals vs predictor have a bullhorn shape, i.e. the varaition in the residuals increases as the living area increases. The model is not suitable.

```{r SLR, echo=F,message=F,warning=F,fig.cap="Model diagnostics Sale Price ~ GrLivArea"}
options(scipen = 0)
trainFiltered <- train %>% 
  dplyr::filter(GrLivArea < 4000 & GarageArea < 1000)

SLR_LivingArea <- lm(data = trainFiltered,SalePrice ~ GrLivArea)
print(summary(SLR_LivingArea), caption = 'ANOVA Simple Linear Regression Above grade living area')

# Model diagnostics
# Tidy store of model results
RedDf <- broom::augment(SLR_LivingArea)
# Plot of residuals
layout(matrix(c(1,2,3,3), 2, 2, byrow = TRUE))
hist(RedDf$.resid, main = "Histogram of residuals", xlab = "Residuals")
qqnorm(RedDf$.resid,title = "Normal Q-Q plot of residuals (Sale Price)")
qqline(RedDf$.resid)
plot(RedDf$GrLivArea,RedDf$.resid,main = "Residuals vs Living Area", xlab = "Above grade living area", ylab = "Residuals")
SLR_LivingArea_NoIntercept <- lm(data = trainFiltered, SalePrice ~ GrLivArea +0)
```  

\pagebreak

### 4.1.1 Non - intercept model with Above grade living area as predictor


\pagebreak
  
    
    
\newpage


\appendix
\begin {center}
\section {APPENDIX}
\end {center}


## A.1 Data quality check

Tables below shows the summary statisics of the numeric variables and it is noted that statistics are within reasonable bounds and appear to be in the units of measure as described in the data dictionary with only 2 rows missing. Also shown are the number of levels or categories in the nominal variables and the number of missing data (0 missing). The data is deemed usable.

```{r Data Quality, echo=F,warning=F,message=F,tidy=T}
library(mosaic)
sanitycheck <- do.call(rbind,dfapply(SampleFrame,favstats, select = is.numeric))
sanitycheck$mean <- as.numeric(format(sanitycheck$mean , digits = 1, nsmall = 2))
sanitycheck$sd <- as.numeric(format(sanitycheck$sd , digits = 1, nsmall = 2))
knitr::kable(sanitycheck, caption = "Data sanity check for numeric variables") 
sanitycheckcharacter <-select(SampleFrame, colnames(SampleFrame[1,sapply(SampleFrame,class) == 'character']))

library(purrr)
UniqueVals <- sanitycheckcharacter %>% 
  map(unique)
#s <- data.frame(names(tst),sapply(tst,function(x){paste(x,collapse = ",")}),row.names = NULL)
Counts <- data.frame(sapply(UniqueVals,length),
                     do.call(rbind,dfapply(sanitycheckcharacter,length,select = is.character)),do.call(rbind,dfapply(sanitycheckcharacter,n_missing,select = is.character)),row.names = names(UniqueVals))
colnames(Counts) <- c( "# Unique", "n","missing")

knitr::kable(Counts, caption = "Data sanity check for nominal variables",align = c("l","r","r","r")) 
```

\pagebreak

## A.2 R code

```{r, ref.label= knitr::all_labels(), echo=TRUE, eval=FALSE, tidy = T}

```

