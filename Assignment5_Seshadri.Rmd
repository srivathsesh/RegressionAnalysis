---
title: 'Assignment 5: Automated Variable selection, multicollinearity and predictive
  modeling.'
author: "Sri Seshadri"
date: "7/19/2017"
output: 
  pdf_document: 
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

## 1. Introduction
<span> <style color = 'red'> NEEDS TO BE COMPLETED </style> </span>

```{r Read data, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE}
ames <- readr::read_delim(file = 'ames_housing_data.csv', delim = ",")
# chamge from scientic notations, to restore to default options(scipen = 0)
options(scipen = 999)
library(magrittr)
```

```{r Sample Frame, echo = FALSE, message=FALSE, warning=FALSE, tidy=TRUE}
LivingAreaCutoff <- 800
# Adding drop conditions varaible
# insert dummy variable to code SaleCondition being either equal to 'Normal' or 'Partial'
ames$Sale_NrmPar <- ifelse(ames$SaleCondition == 'Normal' | ames$SaleCondition == 'Partial', 1,0)
ames$DropCondition <- ifelse(ames$Zoning!='RL','01: Not LowDensityZone',
	ifelse(ames$Sale_NrmPar == 0,'02: Not Normal/Partial Sale',
	ifelse(ames$Street!='Pave','03: Street Not Paved',
	ifelse(ames$GrLivArea <LivingAreaCutoff,'04: Less than 800 SqFt',
	 ifelse(ames$TotalBsmtSF <1,'05: No Basement', 
	 ifelse(ames$GrLivArea > 4000, '06: Greater 4000 sqft living Area - Influence Points',
ifelse(ames$GarageArea > 1000, '07:Garage area greater than 1000 sqft - Influence points',
	'99: Eligible Sample')
	))))))

# Waterfall
waterfall <- ames %>% 
  dplyr::group_by(DropCondition) %>% 
  dplyr::summarise(counts=n())

```

## 2. Sample definition

It is assumed that typical home buyers are those that move from apartments to single family or town homes. Also apartments are less likely to be sold to individuals as they remain holdings of owners for rental income. Single family and town homes belong to "Residential Low density" (RL) zoning classification in the city of Ames. Data belonging to only to the RL zone is considered for analysis and model development. Also, it is assumed that typical homes have paved streets for access and above grade living area greater than `r LivingAreaCutoff` square feet. Sales data belonging to homes that were sold in abnormal conditions such as trade in, foreclosure or short sale are not included in the analysis. Also, sales between family members, sale of adjoining lot, linked properties are omitted from the data. Homes with no basements are excluded from the analysis at this time. Homes with living area greater than 4000 square feet and garage area greater than 1000 square feet were identified as outliers and are therefore removed from the analysis. Table 1 shows the waterfall of the data not included in the data and the eligible samples.

```{r Waterfall, echo=FALSE, warning = F, message=F, tidy=T}
# Print waterfall table
knitr::kable(waterfall, align = c("l", "r"),caption = "Drop waterfall")
# Define training portion of the data
trainPercent <- round(0.7,1)
# Columns of interest
colsofinterest <- c('SID'
  ,'LotArea'
, 'LotConfig'
, 'Neighborhood'
, 'BldgType'
, 'OverallCond'
, 'YearRemodel'
, 'TotalBsmtSF'
, 'GrLivArea'
, 'BsmtFullBath'
, 'BsmtHalfBath'
, 'FullBath'
, 'HalfBath'
, 'BedroomAbvGr'
, 'KitchenQual'
, 'TotRmsAbvGrd'
, 'GarageArea'
, 'MoSold'
, 'YrSold'
, 'SaleCondition'
, 'FirstFlrSF'
, 'SecondFlrSF'
, 'OverallQual'
, 'SalePrice')

```  

### 2.1 Predictor variables of interest for modelling

The following variables in the data were deemed to be of interest for model building. The choice of parameters was based upon intial Exploratory Data Analysis (EDA) and subject matter expertise. The categorical variables are coded into indicator variables for model building purposes, the tables 3, 4, 5 and 6 show the description of the indicator variables.

```{r SampleFrame, echo = F, warning = F, message=F, tidy=T}
# Cleanly show the columns of interest in pdf. Making the colsofinterest as matrix for easy printing
# printing on pdf
knitr::kable(matrix(c(colsofinterest[2:23],"",""),ncol = 3,byrow = F), caption = "Predictors of interest")
tierdf <- data.frame(Tier = c(1,2,3,4), `Price per sq.ft` = c("<= 60", "> 60 and <= 70", "> 70 and <= 80", "> 80 and <= 90"))
Lotconfigdf <- data.frame(Indicator = c("CornerLot", "CulDSac", "Frontal2", "Frontal3"), Description = c("Corner lot", "CulDSac Lot", "2 frontal lot", "3 frontal lot") )
Bldgtypedf <- data.frame(Indicator = c("TwnhsE","Twnhs","Duplex","fam2"),Decription = c("Townhouse", "Twin house", "Duplex", "2 family conversion"))
KitchenQualdf <- data.frame(Indicator = c("KTA", "KGD", "KEx", "KFa"),Decription = c("Typical/Average", "Good", "Excellent", "Fair"))
Predictors <- c('LotArea'
                ,'YearRemodel'
                , 'TotalBsmtST'
                , 'GrLivArea'
                , 'TotalBath'
                , 'TotalSQFT'
                , 'QualityIndex'
                , 'TotRmsAbvGrd'
                , 'GarageArea'
                , 'YearMonthSold'
                , 'Tier1'
                , 'Tier2'
                , 'Tier3'
                , 'Tier4'
                , 'Tier5'
                , 'PartialSaleCond'
                , 'CornerLot'
                , 'CulDSac'
                , 'Frontal2'
                , 'Frontal3'
                , 'TwnhsE'
                , 'Twnhs'
                , 'Duplex'
                , 'fam2'
                , 'KTA'
                , 'KGD'
                , 'KEx'
                , 'KFa'
)
```  

The following tables describe the indicator variables:

```{r IV, echo=F,warning=F,message=F,tidy=T}
knitr::kable(tierdf, caption = "Neighborhood tiers,base category > 90")
knitr::kable(Lotconfigdf, caption = "Lot configuration indicator variables; base category: Inside Lot")
knitr::kable(Bldgtypedf, caption = "Building type indicator variables; base category: single family")
knitr::kable(KitchenQualdf, caption = "Kitchen Quality indicator variables; base category: poor")
```




### 2.2 Training and validation samples.

From the eligible samples, `r paste0(trainPercent*100,"%")` of the data is randomly sampled to be used as the dataset for model development. This dataset would be refered to as training dataset. The remaining `r paste0((1-trainPercent)*100,"%")` is used as the validation set to evaluate the model performance of predicting sale price on data that is outside the training set. Table 7 shows the split of the total eligible samples.  


```{r TrainingSet, echo=F, warning=F, message=F, tidy=T}
# Get sample frame.
SampleFrame <- ames %>% 
  dplyr::filter(DropCondition == '99: Eligible Sample') %>% 
  dplyr::select_(.dots = colsofinterest)
SampleFrame <- SampleFrame %>% 
  dplyr::mutate(TotalBath = BsmtFullBath + BsmtHalfBath + FullBath + HalfBath) %>% 
  dplyr::mutate(TotalSQFT = TotalBsmtSF + FirstFlrSF + SecondFlrSF) %>% 
  dplyr::mutate(SQFTNoBsmt = FirstFlrSF + SecondFlrSF) %>% 
  dplyr::mutate(QualityIndex = OverallQual*OverallCond) %>% 
  dplyr::mutate(YearMonthSold = YrSold + MoSold/100) %>% 
  dplyr::mutate(u = runif(nrow(SampleFrame)))
# -----------------------------Grouping Neighborhoods-----------------------------------------------
NeighborhoodGrps <- SampleFrame %>%
              dplyr:: select(Neighborhood,SalePrice,TotalSQFT) %>% 
               dplyr::group_by(Neighborhood) %>% 
              dplyr::summarise(TotalSalePrice = sum(SalePrice), Total.SQFT = sum(TotalSQFT),PricePerSqFt = TotalSalePrice/Total.SQFT)

Less60Neigh <- NeighborhoodGrps$Neighborhood[which(NeighborhoodGrps$PricePerSqFt <= 60)]
Bet60_70 <-  NeighborhoodGrps$Neighborhood[which(NeighborhoodGrps$PricePerSqFt <= 70 & NeighborhoodGrps$PricePerSqFt > 60 )]
Bet70_80 <-  NeighborhoodGrps$Neighborhood[which(NeighborhoodGrps$PricePerSqFt <= 80 & NeighborhoodGrps$PricePerSqFt > 70 )]
Bet80_90 <-   NeighborhoodGrps$Neighborhood[which(NeighborhoodGrps$PricePerSqFt <= 90 & NeighborhoodGrps$PricePerSqFt > 80)]
Greater_90 <-   NeighborhoodGrps$Neighborhood[which(NeighborhoodGrps$PricePerSqFt > 90)]

SampleFrame$Tier1 <- ifelse(SampleFrame$Neighborhood %in% Less60Neigh,1,0)
SampleFrame$Tier2 <- ifelse(SampleFrame$Neighborhood %in% Bet60_70,1,0)
SampleFrame$Tier3 <- ifelse(SampleFrame$Neighborhood %in% Bet70_80, 1,0)
SampleFrame$Tier4 <- ifelse(SampleFrame$Neighborhood %in% Bet80_90, 1,0)
#SampleFrame$Tier5 <- ifelse(SampleFrame$Neighborhood %in% Greater_90, 1,0)

tierdf <- data.frame(Tier = c(1,2,3,4,5), `Price per sq.ft` = c("<= 60", "> 60 and <= 70", "> 70 and <= 80", "> 80 and <= 90", " > 90"))
#-------------------------------------------------------------------------------------------------------

#------------------------------Creating indicator variables --------------------------------------------
# IV for LotConfig, base Inside lot
SampleFrame$CornerLot <- ifelse(SampleFrame$LotConfig == "Corner",1,0)
SampleFrame$CulDSac <- ifelse(SampleFrame$LotConfig == "CulDSac",1,0)
SampleFrame$Frontal2 <- ifelse(SampleFrame$LotConfig == "FR2",1,0)
SampleFrame$Frontal3 <- ifelse(SampleFrame$LotConfig == "F3",1,0)
Lotconfigdf <- data.frame(Indicator = c("CornerLot", "CulDSac", "Frontal2", "Frontal3"), Description = c("Corner lot", "CulDSac Lot", "2 frontal lot", "3 frontal lot") )
# IV for BlgType, base "1Fam"
SampleFrame$TwnhsE <- ifelse(SampleFrame$BldgType == "TwnhsE",1,0)
SampleFrame$Twnhs <- ifelse(SampleFrame$BldgType == "Twnhs", 1,0)
SampleFrame$Duplex <- ifelse(SampleFrame$BldgType == "Duplex", 1,0)
SampleFrame$fam2 <- ifelse(SampleFrame$BldgType == "2famCon",1,0)
Bldgtypedf <- data.frame(Indicator = c("TwnhsE","Twnhs","Duplex","fam2"),Decription = c("Townhouse", "Twin house", "Duplex", "2 family conversion"))
# IV for SaleCondition, bae "Normal"
SampleFrame$PartialSaleCond <- ifelse(SampleFrame$SaleCondition == "Partial",1,0)
# IV for KitchenQual
SampleFrame$KTA <- ifelse(SampleFrame$KitchenQual == "TA", 1,0)
SampleFrame$KGD <- ifelse(SampleFrame$KitchenQual == "Gd", 1,0)
SampleFrame$KEx <- ifelse(SampleFrame$KitchenQual == "Ex", 1,0)
SampleFrame$KFa <- ifelse(SampleFrame$KitchenQual == "Fa", 1,0)
KitchenQualdf <- data.frame(Indicator = c("KTA", "KGD", "KEx", "KFa"),Decription = c("Typical/Average", "Good", "Excellent", "Fair"))
# --------------------------------------------------------------------------------------------------------------
Predictors <- c('LotArea'
                ,'YearRemodel'
                , 'TotalBsmtSF'
                , 'GrLivArea'
                , 'TotalBath'
                , 'TotalSQFT'
                , 'QualityIndex'
                , 'TotRmsAbvGrd'
                , 'GarageArea'
                , 'YearMonthSold'
                , 'Tier1'
                , 'Tier2'
                , 'Tier3'
                , 'Tier4'
                , 'PartialSaleCond'
                , 'CornerLot'
                , 'CulDSac'
                , 'Frontal2'
                # , 'Frontal3'
                , 'TwnhsE'
                , 'Twnhs'
                , 'Duplex'
            #,'fam2'
                , 'KTA'
                , 'KGD'
                , 'KEx'
                # , 'KFa'
)
# training set
#train <- dplyr::sample_n(SampleFrame,size = trainPercent*nrow(SampleFrame), replace = F,set.seed(2000))
set.seed(200)
train <- subset(SampleFrame , u < 0.7)
train <- train %>% dplyr::arrange(SID)
train.Clean <- train %>% dplyr::select(c(Predictors,'SalePrice'))
# Validation set
#Validation <- dplyr::sample_n(SampleFrame,size = (1-trainPercent)*nrow(SampleFrame), replace = F, set.seed(2000))
Validation <- subset(SampleFrame, u > 0.7)
Validation <- Validation %>% dplyr::arrange(SID) 
# Check row counts
df <- cbind(Data = c("Training set", "Validation set","Total"), Samples = c(nrow(train),nrow(Validation),nrow(train) + nrow(Validation)))
knitr::kable(df,align = c("l","r"),caption = "Training and Validation sampling")

```

## 3. Model identification by Automated Variable Selection

Attribute variables such as "TotalSQFT"; sum of basement square feet and above grade living area and "QualityIndex"; product of overall quality and overall condition has been created for use as predictor variables in model building. With the attribute variables and the indicator variables created, below is a list of predictor variables that will be used for automated variable selection and modelling.

```{r Predictors, echo=F, warning=F,message=F,tidy=T}
knitr::kable(matrix(Predictors,ncol = 4), caption = "Predictors for linear regression models")
```

### 3.1. Full model

```{r FullModel, echo=F,message=F,warning=F,tidy=T}
# upper model as full model
upper.lm <- lm(data = train.Clean,SalePrice ~ .)
coefs <- round(coefficients(upper.lm),2)
signs <- ifelse(sign(coefs)==1,"+", "-")
Betas <- paste(abs(coefs[2:length(coefs)]),"*",Predictors)
uppereqn <- paste("SalePrice = ",paste(coefs[1],paste(paste(signs[2:24], Betas),collapse = " ")))

```

A full model with all the predictors used is fit as an exhaustive search. This will be used as an upper boundry for automated variable selection processes; forward selection and as a starting condition in the backward selection process. The full model and its fit is shown below:    

__`r uppereqn`__  

```{r PrintFull_NoInterceptModel, echo=F,warning=F,tidy=T}
options(scipen = 0)
summary(upper.lm)
# lower model as the intercept model
lower.lm <- lm(data = train.Clean, SalePrice ~ 1)
```

### 3.2. Intercept only model as start for forward variable selection

An intercept only model is used as a lower boundary (no predictors used) condition for forward selection.  


\begin {center}
`r paste("Sale Price = ", round(coef(lower.lm),2))`
\end {center}  

  
  
```{r InterceptModel, echo=F,message=F,warning=F,tidy=T}
# print summary for InterceptModel
summary(lower.lm)
# model to initate a stepwise regression
sqft.lm <- lm(data = train.Clean, SalePrice ~ TotalSQFT)
coef_IM <- round(coef(sqft.lm),2)
signs_IM <- ifelse(sign(coef_IM)==1,"+","-")
Betas_IM <- paste(abs(coef_IM[2:length(coef_IM)]),"*","TotalSQFT")
lowereqn <- paste("SalePrice = ",paste(coef_IM[1],paste(paste(signs_IM[2:2], Betas_IM),collapse = " ")))
```
\pagebreak


### 3.3. Simple linear regression as starting condition for stepwise variable selection

A simple linear regression model with total square feet as predictor is used as a starting point for stepwise variable selection.Below is the model and fit:  


\begin {center}
*`r lowereqn`*
\end {center}


```{r step, echo=F,message=F,warning=F,tidy=T}
 summary(sqft.lm)
# forward selection
forward.lm <- MASS::stepAIC(object = lower.lm,scope = list(upper = formula(upper.lm), lower = ~1), direction = "forward",trace = F)
coef_Fwd <- round(coef(forward.lm),2)
signs_Fwd <- ifelse(sign(coef_Fwd)==1,"+","-")
formula_Fwd <- as.character(formula(forward.lm))[3]
predictors_Fwd <- unlist(strsplit(formula_Fwd, split = "+",fixed = T))
Betas_Fwd <- paste(abs(coef_Fwd[2:length(coef_Fwd)]),"*",predictors_Fwd)
fwdeqn <- paste("SalePrice = ",paste(coef_Fwd[1],paste(paste(signs_Fwd[2:length(signs_Fwd)], Betas_Fwd),collapse = " ")))
```  

## 3.4. Forward variable selection.

A forward selection method is used for selecting variables for linear regression fit. The following model was fit by the automated forward selection by AIC.

 __`r fwdeqn`__
```{r Forward,echo=F,message=F,warning=F,tidy=T}
options(scipen = 0)
summary.lm(forward.lm)
# Backward selection
backward.lm <- MASS::stepAIC(object = upper.lm, direction = "backward",trace = F)
coef_Bwd <- round(coef(backward.lm),2)
signs_Bwd <- ifelse(sign(coef_Bwd)==1,"+","-")
formula_Bwd <- as.character(formula(backward.lm))[3]
predictors_Bwd <- unlist(strsplit(formula_Bwd, split = "+",fixed = T))
Betas_Bwd <- paste(abs(coef_Bwd[2:length(coef_Bwd)]),"*",predictors_Bwd)
Bwdeqn <- paste("SalePrice = ",paste(coef_Bwd[1],paste(paste(signs_Bwd[2:length(signs_Bwd)], Betas_Bwd),collapse = " ")))
```

## 3.5 Backward elimination selection.

The following model is a result of a backward elimination selection, with full model as its starting model.

 __`r Bwdeqn`__
 
 
```{r Backward, echo=F,message=F,warning=F,tidy=T}
summary.lm(backward.lm)
# stepwise 
stepwise.lm <- MASS::stepAIC(object = sqft.lm,scope = list(upper = formula(upper.lm), lower = ~1),direction = "both", trace = F)
coef_Stp <- round(coef(stepwise.lm),2)
signs_Stp <- ifelse(sign(coef_Stp)==1,"+","-")
formula_Stp <- as.character(formula(stepwise.lm))[3]
predictors_Stp <- unlist(strsplit(formula_Stp, split = "+",fixed = T))
Betas_Stp <- paste(abs(coef_Stp[2:length(coef_Stp)]),"*",predictors_Stp)
Stpeqn <- paste("SalePrice = ",paste(coef_Stp[1],paste(paste(signs_Stp[2:length(signs_Stp)], Betas_Stp),collapse = " ")))
```

## 3.6. Stepwise regression method.

The following model is a result of stepwise variable selection with full model and simple linear regression model with Total square feet as a predictor as boundary conditions. 

 __`r Stpeqn`__
 
```{r Stepwise, echo=F,message=F,warning=F,tidy=T}
summary(stepwise.lm)
#junkmodel
junk.lm <- lm(SalePrice ~ OverallQual + OverallCond + QualityIndex + GrLivArea + TotalSQFT, data = train)
coef_Jnk <- round(coef(junk.lm),2)
signs_Jnk <- ifelse(sign(coef_Jnk)==1,"+","-")
formula_Jnk <- as.character(formula(junk.lm))[3]
predictors_Jnk <- unlist(strsplit(formula_Jnk, split = "+",fixed = T))
Betas_Jnk <- paste(abs(coef_Jnk[2:length(coef_Jnk)]),"*",predictors_Jnk)
Jnkeqn <- paste("SalePrice = ",paste(coef_Jnk[1],paste(paste(signs_Jnk[2:length(signs_Jnk)], Betas_Jnk),collapse = " ")))
```
## 3.7. Junk Model

The attempt here is to verify the intuition of how correlated predictor variables perform. And how they compare to models from automated variable selection. The predictors chosen are OverallQual,OverallCond, QualityIndex, GrLivArea and TotalSQFT. QualityIndex is a linear combination of OverallQual and OverllCond. TotalSQFT is a result of a linear combination of variables that include GrLivArea. Below is the junk model and its fit.

 __`r Jnkeqn`__
 
```{r junkModel, echo=F,message=F,warning=F,tidy=T}

summary(junk.lm)

```
### 3.7.1 Variance Inflation Factors (VIF) 

Having known that the predictors in the junk model are correlated, it would be interesting to compare correlation amongst predictors in the models from automated variable selection methods against that of the junk model. Table 9 shows the top 5 highest VIFs by models. 

It is not surprising that the highest VIFs are from the junk model. However we see neighborhood Tiers 2 and 3 have high VIF ( > 10). This is not surprising either as they are indicator(dummy) variables. The indicator variables are correlated amongst themselves and can be modelled as linear combinations of each other. Thereby yielding a high coefficient of determintion when indicator variables are regressed by other predictors, causing an inflated VIF. 

```{r VIFs, echo=F,warning=F,message=F,tidy=T}
# VIF

JunkVIF <- as.data.frame(sort(car::vif(junk.lm),decreasing = T))
JunkVIF$Model <- "Junk"
JunkVIF$Predictors <- row.names(JunkVIF)
colnames(JunkVIF) <- c("VIF","Model", "Predictors")

forwardVIF <- as.data.frame(sort(car::vif(forward.lm),decreasing = T))
forwardVIF$Model <- "Forward Selection"
forwardVIF$Predictors <- row.names(forwardVIF)
colnames(forwardVIF) <- c("VIF","Model", "Predictors")
backwardVIF <- as.data.frame(sort(car::vif(backward.lm),decreasing = T))
backwardVIF$Model <- "Backward Elimination"
backwardVIF$Predictors <- row.names(backwardVIF)
colnames(backwardVIF) <- c("VIF","Model", "Predictors")
stepwiseVIF <- as.data.frame(sort(car::vif(stepwise.lm),decreasing = T))
stepwiseVIF$Model <- "Stepwise"
stepwiseVIF$Predictors <- row.names(stepwiseVIF)
colnames(stepwiseVIF) <- c("VIF","Model", "Predictors")
VIF <- rbind(JunkVIF,forwardVIF,backwardVIF,stepwiseVIF,row.names = F)
VIF <- VIF[,c("Model","Predictors", "VIF")]
VIF$VIF <- round(VIF$VIF,2)

VIFReport <- VIF[1:nrow(VIF)-1,] %>% dplyr::group_by(Model) %>% 
  dplyr::top_n(n =5,wt = VIF)
#colnames(VIF) <- c("Predictor","VIF", "Model")
knitr::kable(VIFReport,"latex",caption = "VIF - Top 5 my model") %>% 
  kableExtra::kable_styling(latex_options = "striped") %>% 
  kableExtra::collapse_rows(columns = 1)

```

## 4. Model Comparison
```{r ModelComparison, echo=F,warning=F,message=F,tidy=T}
junkmetrics <- broom::glance(junk.lm)
junkmodel <- broom::augment(junk.lm)
junkmetrics$MAE <- mean(abs(junkmodel$.resid))
junkmetrics$MSE <- mean(junkmodel$.resid ^2)

forwardmetrics <- broom::glance(forward.lm)
forwardmodel <- broom::augment(forward.lm)
forwardmetrics$MAE <- mean(abs(forwardmodel$.resid))
forwardmetrics$MSE <- mean(forwardmodel$.resid ^2)

backwardmetrics <- broom::glance(backward.lm)
backwardmodel <- broom::augment(backward.lm)
backwardmetrics$MAE <- mean(abs(backwardmodel$.resid))
backwardmetrics$MSE <- mean(backwardmodel$.resid ^2)

stepwisemetrics <- broom::glance(stepwise.lm)
stepwisemodel <- broom::augment(stepwise.lm)
stepwisemetrics$MAE <- mean(abs(stepwisemodel$.resid))
stepwisemetrics$MSE <- mean(stepwisemodel$.resid ^2)

comparison <- rbind(junkmetrics,forwardmetrics,backwardmetrics,stepwisemetrics)
comparison$model <- c("Junk", "Forward Selection", "Backward Elimination", "Stepwise")
comparison <- comparison %>% 
  dplyr::select(model,adj.r.squared,AIC,BIC,MSE, MAE)

knitr::kable(comparison,caption = "Model comparison")
```

```{r PredictiveAccuracy, echo=F,warning=F,message=F}
forward.test <- predict(object = stepwise.lm, newdata = Validation)
test <- data.frame(Validation$SalePrice, forward.test)
test <- test %>% dplyr::mutate(error = Validation.SalePrice - forward.test)

```


