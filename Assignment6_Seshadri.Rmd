---
title: "Assignment 6: Principle Components in Predictive Modeling"
author: "Sri Seshadri"
date: "7/27/2017"
output: 
  pdf_document:
   fig_caption: yes
   
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,tidy.opts=list(width.cutoff=70),tidy=TRUE)
library(magrittr)
library(ggplot2)
```

## 1. Introduction

This report discusses the analysis of stock portfolio data to predict log returns of Vanguard large cap index fund using stock indices of 20 companies from various industries. The choice of Principle Component Analysis (PCA) for selecting predictors for predictive modeling is compared with full model (all twenty indices as predictors), arbitary model (random selection of predictors) and backward elimination methods of selection. The Mean Absolute Error (MAE) is used a criteria for evaluating predictive performance. It is seen that the PCA based approach and backward elimination methods perform well and PCA is a good data reduction technique.  

## 2. Exploratory analysis

Since there are number of variables in the data, we start by quantifying the associations amongst them by plotting the correlations. The index VV (Vanguard Index) is of interest and it would be useful to plot the correlation of VV with other variables. Figure 1 shows the correlations of VV with other stock indices. 

```{r echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE, fig.align='center',fig.cap= "Correlations with VV", fig.height=3}
# read data in
stocks <- read.csv(file = 'stock_portfolio.csv', header = T)
# format date & sort chronologically
stocks$Date <- as.Date(stocks$Date,'%d -%b-%y') 
stocks <- stocks %>% 
  dplyr::arrange(Date)
# get log-returns of the data
logreturns <- function(x) {
  log(x[-1]/x[-length(x)])
}
returns <- purrr::map_df(.x = stocks[,-1], .f = logreturns)
# compute correlations table

correlations <- as.data.frame(cor(returns))

# get only VV
library(ggplot2)
library(forcats)
VV_cor <- as.data.frame(correlations$VV[-nrow(correlations)])
colnames(VV_cor) <- "VV"
rownames(VV_cor) <- rownames(correlations)[-21]
ggplot(data = VV_cor, mapping = aes(x = fct_reorder(rownames(VV_cor),VV), y = VV)) + geom_col() + coord_flip() + xlab("Stock Index") + ylab("Correlation") 
#+ ggtitle ("                                       Correlations with VV")

```

### 2.1 Statistical graphic Vs Data visualization

While figure 1 is useful for understanding the VV's correlations with other stock indices, it would be useful to gain insights into how other stock indicies are correlated amongst themselves. From figure 2, it can be noted that higher correlations are on the base of the right triangles formed by the matrx diagonal i.e. indices SLB, WFC, XOM and VV are relatively highly correlated with other indices. Such insights are possible only with visualizations such as this.

It can be seen that indices DPS, MPC and PEP are not significantly correlated with other indices. They are likely to have low Variance Inflation Factors (VIF), as mentioned above GS, XOM and SLB are likely to have higher VIF due to their high correlations with other indices.


```{r, echo=FALSE, fig.align="center", fig.cap="Correlation plot", fig.height=4}
corrplot::corrplot(corr = as.matrix(correlations))
```
\pagebreak

### 2.2 Variance Inflation Factor (VIF)

In the previous section, it was seen that GS, XOM, and SLB were highly correlated with other predictors and they were suspected to have relatively high VIF. One way of assessing the VIF is to fit regression models and assessing VIF. Two models; an arbitary model and a full modelare fit. The VIFs are assesed for the predictor variables in the model and is shown below in table 1. Table 1 shows the top 5 VIFs for each model. It is seen that variance inflation factors are less than the thumb rule of 10, therefore there isn't strong evidence of concern due to multicolinearity. It would be interesting to see how Principle Component Analysis (PCA) would work on this data. 

```{r, echo=FALSE, fig.align="center", fig.height=4}
library(car)
# somemodel
RandomModel <- lm(VV ~ GS+DD+DOW+HON+HUN +JPM + KO + MMM+ XOM, data = returns)
summary(RandomModel)

vifrandom <- as.data.frame(sort(car::vif(RandomModel),decreasing = T))
vifrandom$Model <- "Arbitary model"
vifrandom$Predictors <- row.names(vifrandom)
colnames(vifrandom) <- c("VIF", "Model", "Predictors")

# fullmodel
form <- paste0(colnames(correlations)[-21],collapse = "+")
fullmodel <- lm(VV~., data = returns)
summary(fullmodel)
viffm <- as.data.frame(sort(car::vif(fullmodel), decreasing = T))
viffm$Model <- "Full Model" 
viffm$Predictors <- row.names(viffm)
colnames(viffm) <- c("VIF", "Model","Predictors")
VIF <- rbind(vifrandom, viffm, row.names = F)
VIFReport <- VIF[1:nrow(VIF)-1,] %>% dplyr::group_by(Model) %>% 
  dplyr::top_n(n =5,wt = VIF)

```
\break

```{r}
VIFReport <- VIFReport[,c("Model","Predictors","VIF")]
knitr::kable(VIFReport, format = "latex",caption = "Top 5 VIF by model") %>% 
  kableExtra::kable_styling(latex_options = "striped") %>% 
  kableExtra::collapse_rows(columns = 1)
```

## 3. Principle Component Analysis (PCA)

The tickers from AA through XOM (not including VV) are transformed using PCA. Note that the tickers need not be standardized as we are going to be using the log returns of the tickers. The scatter plot of loadings (loadings are the weights of the variables that contribute to the component - the columns of the loading matrix correspond to eigen vectors) of the first two principle components is shown in Figure 3. It is seen that the tickers belonging to soft drinks industry are grouped together. They also have a  higher weightage in explaining the variation in the data. 
 
It will be interesting to see how PCA reduced the data in terms of variation.

```{r echo=F, fig.cap= "Variable loadings of first 2 principle components"}
returns.pca <- princomp(returns[,-21], cor = T)
pc.1 <- returns.pca$loadings[,1]
pc.2 <- returns.pca$loadings[,2]
pcs <- data.frame(pc.1,pc.2,names(pc.1))
colnames(pcs) <- c("PC1","PC2","Index")
pcs$Industry <- c("Indus Metal"
                  , "Banking"
                  , "Oil Field Services"
                  , "Oil Refining"
                  , "Industrial Chemical"
                  , "Industrial Chemical"
                  , "Soft Drinks"
                  , "Banking"
                  , "Oil Field Services"
                  , "Oil Refining"
                  , "Manufacturing"
                  , "Industrial Chemical"
                  , "Banking"
                  , "Soft Drinks"
                  , "Manufacuring"
                  , "Oil Refining"
                  ,"Soft Drinks"
                  , "Oil Field Services"
                  , "Banking"
                  , "Oil Refining"
                  )
ggplot(data = pcs,mapping = aes(x = PC1,y = PC2,label = Index,color = Industry)) + geom_point() + geom_text(size = 3) + theme_bw() + xlim(c(-0.27,-0.12)) + ylim(c(-0.27,0.6))
```

### 3.1 Choosing number of principle components

The choice of number of principle components are to be based on how much of the variance in the variables do the components cumulatively explain.

* Rule of thumb is to choose components that explain 70% to 90% of the variation in the variables.
* Another option is to choose components that have eigenvalues greater than the average of eigen values. If eigenvalues are extracted from correlation matrix, components with eigenvalues less than 1 are excluded.
* Use Scree diagram, a plot of eigenvalues or component variance againt the component number. The number of components selected the is the value corresponding to the location where, the Scree plot forms an elbow.

### 3.1.1 Scree plot

Scree plots are useful in identifying the principle components that contribute to explaining the variation in the variables. Figures 4, 5 and 6 show different variations of the scree plot. Figure 4 is the default version of scree plot in R. The scree plot looks to be truncated to 10 components.It is seen that component 1 explains the majority of the variance. 

It would be useful to see the proportion of variance explained by the components. Figure 5 shows the proportion of the variance that the principle components are able to explain. It is seen that the contribution of principle components to explanation of variance plateaus at component number 8. Additional components beyond 8 are not a significant addition to explaining the variance. As a rule of thumb, principle components that explain 80% (or 70% to 90%) of the total variance is chosen for data reduction. It will be useful to plot the cumulative variance of the principle components. Figure 6 shows the first 8 components explain 80% of the total variance in the data. 

```{r Scree, echo=F,warning=F,message=F,fig.cap="Scree Plot - Not so good looking"}
plot(returns.pca, xlab = "Component Number", main = " ")
# screeplot(returns.pca)
```

```{r Scree2, fig.cap="Scree plot - Variance explained "}
varcontribution <- returns.pca$sdev^2 / sum(returns.pca$sdev^2)
plot(varcontribution, xlab = "Component Number", ylab = "Variance Contribution", type = "l") 
  points(varcontribution)
```

```{r Scree3, fig.cap= "Scree cumulative variance explained"}
cumvariance <- cumsum(returns.pca$sdev^2) / sum(returns.pca$sdev^2)
plot(cumvariance, xlab = "Component Number", ylab = "Cumulative Variance", type = "l")
points(cumvariance)
abline(h=0.8,lwd = 1.5,col = "red")
abline(v = 8,lwd = 1.5, col = "red")
text(13,0.7, 'Keep 8 Principal Components')
```

With the data reduced to eight principle components, the eight components are to be used as predictors for modelling. The next section discusses the model. $$Score_i = a_{i1}(x_1 - \bar{x_1}) + a_{i2}(x_2 - \bar{x_2}) + ...$$



### 3.2. Predictive model with Principle Components as predictors

The scores of the principle components are used as predictors. Score is calculated by multiplying the variable values minus the mean by the "loading"" of the variable. 

We will split the scores into training and test sets for modelling. 70% of the data is chosen in random as the training set and the remaining of data is used as validation set. Ideally, the principle components' eigenvectors are computed based on the training sample and is used to compute the principle components of the validation or test sample. For simplicity, in this assignment the priciple components are calculated using the entire data set. 

```{r split}
set.seed(200)
 returns.scores <- as.data.frame(returns.pca$scores)
 returns.scores$VV <- returns$VV
 returns.scores$u <- runif(n = nrow(returns.scores), min = 0, max = 1)
 
 train.scores <- subset(returns.scores,u < 0.7)
 test.scores <- subset(returns.scores, u >= 0.7)
 
 df <- cbind(Data = c("Training set", "Validation set","Total"), Samples = c(nrow(train.scores),nrow(test.scores),nrow(train.scores) + nrow(test.scores)))
knitr::kable(df,align = c("l","r"),caption = "Training and Validation sampling")

# Linear Regression with PCA
pca.lm <- lm(VV ~ Comp.1 + Comp.2 + Comp.3 + Comp.4 + Comp.5 + Comp.6 + Comp.7 + Comp.8, data = returns.scores)
coef_pca <- round(coef(pca.lm),5)
signs_pca <- ifelse(sign(coef_pca)==1,"+","-")
formula_pca <- as.character(formula(pca.lm))[3]
predictors_pca <- unlist(strsplit(formula_pca, split = "+",fixed = T))
Betas_pca <- paste(abs(coef_pca[2:length(coef_pca)]),"*", predictors_pca)
pcaeqn <- paste("VV = ",paste(coef_pca[1],paste(paste(signs_pca[2:length(signs_pca)], Betas_pca),collapse = " ")))
```


### 3.2.1 Linear Regression with principle components as predictors

A linear regression model with the 8 principle component scores as perdictors is fit. The model and the fit is summarized below. Let us call the model, "PCA model".

__`r pcaeqn`__


```{r Regression}

summary(pca.lm)
vifpc <- as.data.frame(vif(pca.lm))
vifpc$PC <- row.names(vifpc)
colnames(vifpc) <- c("VIF", "PC")
row.names(vifpc) <- NULL
knitr::kable(vifpc[,c("PC","VIF")], caption = "VIF of Principle components")
```

It is seen that VIF for the components are 1. This is not surprising because the principle components are by design are orthogonal to each other. The eigenvectors are constrained to be uncorrelated to each other. 

### 3.2.2. Predictive metric (MAE) for PCA model 

The table below shows the predictive performance of the PCA model. The training and test sample's mean absolute error (MAE) are close.

```{r MAE}
pca.lm.MAE <- mean(abs(pca.lm$residuals))
pca.test <- predict(pca.lm,newdata = test.scores)
pca.test.MAE <- mean(abs(test.scores$VV - pca.test))

MAEdf <- data.frame(Model = "PCA model", Train.MAE = pca.lm.MAE, Test.MAE = pca.test.MAE)
knitr::kable(MAEdf, caption = "MAE of PCA model")
```

## 3.3. Linear regression models with tickers as predictors

It will be interesting to compare the performance of the "PCA model" with linear regression models using raw tickers as the predictors. We will compare the PCA models by fitting an arbitarty model and a full model like the models in section 2.2. To compare the arbitary and full models with PCA model, it would be useful to split the data set into training and validation samples.

```{r}
set.seed(200)
returns$u <- returns.scores$u
returns.train <- subset(returns,u >= 0.7)
returns.test <- subset(returns, u < 0.7)
# df <- cbind(Data = c("Training set", "Validation set","Total"), Samples = c(nrow(returns.train),nrow(returns.test),nrow(returns.train) + nrow(returns.test)))
# knitr::kable(df,align = c("l","r"),caption = "Training and Validation sampling of Raw returns")

# Arbitary model
model.1 <- lm(VV~ GS+DD+DOW+HON+HUN +JPM + KO + MMM+ XOM, data = returns.train)
coef_model.1 <- round(coef(model.1),5)
signs_model.1 <- ifelse(sign(coef_model.1)==1,"+","-")
formula_model.1 <- as.character(formula(model.1))[3]
predictors_model.1 <- unlist(strsplit(formula_model.1, split = "+",fixed = T))
Betas_model.1 <- paste(abs(coef_model.1[2:length(coef_model.1)]),"*", predictors_model.1)
model.1eqn <- paste("VV = ",paste(coef_model.1[1],paste(paste(signs_model.1[2:length(signs_model.1)], Betas_model.1),collapse = " ")))
```

### 3.3.1. Arbitary model

The results for the arbitary model is shown below.

__`r model.1eqn`__

```{r}
#model.1 <- lm(VV~ GS+DD+DOW+HON+HUN +JPM + KO + MMM+ XOM, data = returns.train)
summary(model.1)
model1.train.MAE <- mean(abs(model.1$residuals))
model1.test <- predict(model.1, newdata = returns.test)
model1.test.MAE <- mean(abs(model1.test - returns.test$VV))

ArbMAEdf <- data.frame(Model = "Arbitary model", Train.MAE = model1.train.MAE, Test.MAE = model1.test.MAE)
knitr::kable(ArbMAEdf, caption = "MAE of Arbitary model")

# full model
model.2 <- lm(VV~ ., data = returns.train[,-22])
coef_model.2 <- round(coef(model.2),5)
signs_model.2 <- ifelse(sign(coef_model.2)==1,"+","-")
formula_model.2 <- as.character(formula(model.2))[3]
predictors_model.2 <- unlist(strsplit(formula_model.2, split = "+",fixed = T))
Betas_model.2 <- paste(abs(coef_model.2[2:length(coef_model.2)]),"*", predictors_model.2)
model.2eqn <- paste("VV = ",paste(coef_model.2[1],paste(paste(signs_model.2[2:length(signs_model.2)], Betas_model.2),collapse = " ")))
```

### 3.3.2 Full model

A full model with all tickers as predictors is fit. The model and the results are shown below.

__`r model.2eqn`__

```{r}
summary(model.2)

model2.train.MAE <- mean(abs(model.2$residuals))
model2.test <- predict(model.2, newdata = returns.test)
model2.test.MAE <- mean(abs(model2.test - returns.test$VV))

FullMAEdf <- data.frame(Model = "Full model", Train.MAE = model2.train.MAE, Test.MAE = model2.test.MAE)
knitr::kable(FullMAEdf, caption = "MAE of Full model")
```

### 3.3.3 Model comparison

THe models that have been fit so far is shown in the table below. 
```{r}
models <- data.frame(Models = c("PCA","Arbitary","Full"), Equation = c(pcaeqn,model.1eqn,model.2eqn))
pander::pandoc.table(models, "Model equations",justify = "left",emphasixe.cols = 2)

```

The predicitve perfromance of the models are compared in table 8. It is seen that from a predictive performance stand point the PCA model is the best model; it has the lowest MAE. Also, from a VIF perspective the PCA model has the lowest VIF possible, which makes the regression coefficients stable. This reflects in the predictive performance, especially when there test sample has minor extrapolation in the predicitve space compared to the training sample.

It is seen that not all principle components' coefficiennts where statistically signifiant, it'll be useful to see how automated variable selection methods choose principle components as predictors. PCA being an unsupervised variable reduction technique, it'll be useful to wrap this technique with supervised (variable selection with response variable) automated variable selection technique. 
```{r}
ModelComp <- rbind(MAEdf,ArbMAEdf,FullMAEdf)
knitr::kable(ModelComp, caption = "Predictive performance of regression models")

# backward elimination
full.lm <- lm(VV ~ ., data = train.scores)
#summary(full.lm)

library(MASS)
backward.lm <- stepAIC(full.lm,direction = c('backward'),trace = F)


coef_backward.lm <- round(coef(backward.lm),5)
signs_backward.lm <- ifelse(sign(coef_backward.lm)==1,"+","-")
formula_backward.lm <- as.character(formula(backward.lm))[3]
predictors_backward.lm <- unlist(strsplit(formula_backward.lm, split = "+",fixed = T))
Betas_backward.lm <- paste(abs(coef_backward.lm[2:length(coef_backward.lm)]),"*", predictors_backward.lm)
backward.lmeqn <- paste("VV = ",paste(coef_backward.lm[1],paste(paste(signs_backward.lm[2:length(signs_backward.lm)], Betas_backward.lm),collapse = " ")))

```

## 4. Automated variable selection

The backward elimination variable selection method is employed to choose predictors from the 20 priciple components. The fit for the backward elimination is shown below.

__`r backward.lmeqn`__

The backward elimination method picked 10 of the 20 principle components compared to the 8 that was picked in the PCA analysis in section 3.1. The backward elimination chose components 9, 10,11,14 and 16 in place of Components 5,6, and 7 in section 3.1. The variance inflation factors reamin at 1 which is not surprising, for reasons mentioned in section 3.2.1. 

```{r}

summary(backward.lm)
vif(backward.lm)


```

### 4.1. Model comparison

The predictive performance of the models fitted are compared below. Though the backward elimination's test MAE is better than the rest, its not by a significant margin. The PCA and the backward elimination models are performing alike. 


```{r}
bkward.MAE.train <- mean(abs(backward.lm$residuals))
bkward.test <- predict(backward.lm,newdata = test.scores)
bkward.MAE.test <- mean(abs(bkward.test - test.scores$VV))

BkwardMAEdf <- data.frame(Model = "Backward",Train.MAE = bkward.MAE.train, Test.MAE = bkward.MAE.test)
ModelComp <- rbind(ModelComp,BkwardMAEdf)
knitr::kable(ModelComp,caption = "Comparison of regression models")
```

## 5. Conclusion

The stock portfolio data was analyzed and predictive models for predicting Vanguard Large Cap index with stocks from twenty companies belonging to various industries as predictors were modelled. Several predictor selection approaches like PCA, full model, arbitary model and backward elimination method were used. The predictive performance of the models were compared. It was found that PCA was a good data reduction technique that reduces the number of variables to key principle components for analysis and modeling. The PCA based model and backward elimination model had comparable predictive performance. They performed better than the arbitary and full model.

\pagebreak
 
\appendix
\begin {center}
\section {APPENDIX}
\end {center}


## A.1 R code

```{r, ref.label= knitr::all_labels(), echo=TRUE, eval=FALSE, tidy = T}

```
